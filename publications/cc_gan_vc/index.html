<html>

<head>
  <meta charset="UTF-8">
  <title>Audio samples of "NON-PARALLEL MANY-TO-MANY VOICE CONVERSION USING CROSS-CYCLE CONSISTENCY LOSS"</title>
  <link rel="stylesheet" type="text/css" href="../../stylesheet.css" />
  <!-- <link rel="shortcut icon" href="../../images/taco.png"> -->
</head>

<body>
  <article>
    <header>
      <h1>Audio samples of "NON-PARALLEL MANY-TO-MANY VOICE CONVERSION USING CROSS-CYCLE CONSISTENCY LOSS"</h1>
    </header>
  </article>

  <!-- <div><b>Paper:</b> <a href="https://arxiv.org/abs/1703.10135">arXiv</a> <a href="http://www.interspeech2017.org/program/technical-program/">Interspeech 2017</a></div> -->
  <!-- <div><b>Authors:</b> Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis Agiomyrgiannakis, Rob Clark, Rif A. Saurous</div> -->
  <div><b>Abstract:</b>Recently, the multi-speaker voice conversion(VC) has been successfully used in many tasks. 
    However, this technique suffers from two primary limitations. One is that the parallel data hard to get, 
    the other is that the model can only convert the voice to the speakers contained in the training data. 
    In this paper, we adopt a disentangled adversarial network to zero-shot voice conversion using non-parallel data.
    In our proposed method, content features and attribute features are extracted from inputs in different speaker domains. 
    The content features are obtained from a content adversarial network while the attribute features are represented by a style embedding. 
    The generator synthesizes output voice based on content features and attribute features. Most importantly, 
    a cross-cycle consistency loss makes content and attribute migration possible between non-parallel voice pairs. 
    Objective and subjective evaluations show that the voice that generated by our model is not only similar to the target speaker but also has a better quality.</div>

  <div><p>
    We compare our method with Adaptive-VC (<a href="https://arxiv.org/abs/1904.05742">https://arxiv.org/abs/1904.05742</a>) in zero-shot voice conversion. <b>All four target speakers are choosen from our test set, but we do not know how Adaptive-VC splited the dataset.</b>
    <br>The samples of Adaptive-VC are converted directly by the pretrain model which the author provided. (<a href=https://github.com/jjery2243542/adaptive_voice_conversion>https://github.com/jjery2243542/adaptive_voice_conversion</a>).
  </p></div>

  <div>
      <h3>P293(Female) -> P360(Male)</h3>
      <table>
        <tbody>
          <tr>
            <td><span><b>Source speech example</b></td></span>
            <td><span><b>Target speech example</b></td></span>
            <td><span><b>Adaptive-VC</b></td></span>
            <td><span><b>Proposed</b></td></span>
          </tr>
          <tr>
            <td><audio controls=""><source src="demos/proposed/F2M/p293_053.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/F2M/p360_005.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/baseline/baseline-adaptive-vc/F2M/293-360-053.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/F2M/293_360_053.wav" type="audio/wav"></audio></td>
          </tr>
          <tr>
            <td><audio controls=""><source src="demos/proposed/F2M/p293_117.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/F2M/p360_005.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/baseline/baseline-adaptive-vc/F2M/293-360-117.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/F2M/293_360_117.wav" type="audio/wav"></audio></td>
          </tr>
        </tbody>
      </table>

      <h3>P272(Male) -> P232(Male)</h3>
      <table>
        <tbody>
          <tr>
            <td><span><b>Source speech example</b></td></span>
            <td><span><b>Target speech example</b></td></span>
            <td><span><b>Adaptive-VC</b></td></span>
            <td><span><b>Proposed</b></td></span>
          </tr>
          <tr>
            <td><audio controls=""><source src="demos/proposed/M2M/p272_025.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/M2M/p232_007.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/baseline/baseline-adaptive-vc/M2M/272-232-025.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/M2M/272_232_025.wav" type="audio/wav"></audio></td>
          </tr>
          <tr>
            <td><audio controls=""><source src="demos/proposed/M2M/p272_046.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/M2M/p232_007.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/baseline/baseline-adaptive-vc/M2M/272-232-046.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/M2M/272_232_046.wav" type="audio/wav"></audio></td>
          </tr>
        </tbody>
      </table>

    <h3>P361(Female) -> P229(Female)</h3>
      <table>
        <tbody>
          <tr>
            <td><span><b>Source speech example</b></td></span>
            <td><span><b>Target speech example</b></td></span>
            <td><span><b>Adaptive-VC</b></td></span>
            <td><span><b>Proposed</b></td></span>
          </tr>
          <tr>
            <td><audio controls=""><source src="demos/proposed/F2F/p361_075.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/F2F/p229_005.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/baseline/baseline-adaptive-vc/F2F/361-229-075.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/F2F/361_229_075.wav" type="audio/wav"></audio></td>
          </tr>
          <tr>
            <td><audio controls=""><source src="demos/proposed/F2F/p361_065.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/F2F/p229_005.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/baseline/baseline-adaptive-vc/F2F/361-229-065.wav" type="audio/wav"></audio></td>
            <td><audio controls=""><source src="demos/proposed/F2F/361_229_065.wav" type="audio/wav"></audio></td>
          </tr>
        </tbody>
      </table>
    
    <h3>P232(Male) -> P229(Female)</h3>
    <table>
      <tbody>
        <tr>
          <td><span><b>Source speech example</b></td></span>
          <td><span><b>Target speech example</b></td></span>
          <td><span><b>Adaptive-VC</b></td></span>
          <td><span><b>Proposed</b></td></span>
        </tr>
        <tr>
          <td><audio controls=""><source src="demos/proposed/M2F/p232_033.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="demos/proposed/M2F/p229_005.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="demos/baseline/baseline-adaptive-vc/M2F/232-229-033.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="demos/proposed/M2F/232_229_033.wav" type="audio/wav"></audio></td>
        </tr>
        <tr>
          <td><audio controls=""><source src="demos/proposed/M2F/p232_080.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="demos/proposed/M2F/p229_005.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="demos/baseline/baseline-adaptive-vc/M2F/232-229-080.wav" type="audio/wav"></audio></td>
          <td><audio controls=""><source src="demos/proposed/M2F/232_229_080.wav" type="audio/wav"></audio></td>
        </tr>
      </tbody>
    </table>

</body>

</html>